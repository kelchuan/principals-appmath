{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Marc Spiegelman</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fun with polynomials\n",
    "\n",
    "**GOAL:** Explore the ideas of Interpolation, Least Squares fitting and projection of continous functions onto the function space $P_2[-1,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Space $P_2[-1,1]$\n",
    "\n",
    "Consider the space of all second order polynomials on the closed interval $x\\in[-1,1]$ which is a subspace of continuous functions $C^0[-1,1]$.   To completely describe a vector space we need a basis:  a set of linear independent vectors that span the space.  While there are many possible bases for $P_2[-1,1]$, here we will consider the simplest **monomial** basis $p_0(x)=1$, $p_1(x)=x$, $p_2(x)=x^2$ or\n",
    "\n",
    "$$\n",
    "    P_2(x)[-1,1] = \\mathrm{span}<1,x,x^2>\n",
    "$$\n",
    "\n",
    "i.e. every vector in $P_2$ can be written as a linear combination of the basis vectors as \n",
    "\n",
    "$$\n",
    "    f(x) = c_0p_0 + c_1p_1 + c_2p_2 = c_0 + c_1 x + c_2x^2\n",
    "$$\n",
    "\n",
    "The space P_2(x)[-1,1] is said to be *isomorphic* to $R^3$ as every vector in $P_2$ can be associated with a unique vector in $R^3$ \n",
    "\n",
    "$$\n",
    "    \\mathbf{c}= [ c_0, c_1, c_2]^T\n",
    "$$\n",
    "\n",
    "here we will set up a bit of python to evaluate polynomials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lambda functions for each of the basis functions\n",
    "p0 = lambda x: np.ones(x.shape)\n",
    "p1 = lambda x: x\n",
    "p2 = lambda x: x**2\n",
    "\n",
    "\n",
    "# lambda function for the matrix whose columns are p_i(x)\n",
    "A = lambda x: np.array([ p0(x), p1(x), p2(x)]).transpose()\n",
    "\n",
    "# lambda function for any vector in P_2,  v = c[0]*p0 + c[1]*p1 + c[2]*p2\n",
    "v = lambda c,x :  np.dot(A(x),c)\n",
    "\n",
    "x = np.array([-1.,0.,1.])\n",
    "print p0(x),p1(x),p2(x)\n",
    "\n",
    "print A(x)\n",
    "\n",
    "c = np.array([1,2,-1])\n",
    "print v(c,x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1)\n",
    "plt.figure()\n",
    "plt.plot(x,p0(x),label='$p_0$')\n",
    "plt.hold(True)\n",
    "plt.plot(x,p1(x),label='$p_1$')\n",
    "plt.plot(x,p2(x),label='$p_2$')\n",
    "plt.xlabel('x')\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the *interpolating* polynomial that goes through exactly three points.\n",
    "$f(-1)=0$, $f(0)=2$, $f(1)=-1$ by solving the invertible system of linear equations\n",
    "\n",
    "$$\n",
    "    [\\, p_0(x)\\quad p_1(x)\\quad p_2(x)\\, ] \\mathbf{c} = f(x)\n",
    "$$ \n",
    "\n",
    "for the three points in  $x=[-1,0,1]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-1.,0.,1.])\n",
    "f = np.array([0.,2.,-1.])\n",
    "c = la.solve(A(x),f)\n",
    "\n",
    "# and plot it out\n",
    "xx = np.linspace(-1,1) # use well sampled space for plotting the quadratic\n",
    "plt.figure()\n",
    "# plot the parabola\n",
    "plt.plot(xx,v(c,xx),'r-')\n",
    "# plot the interpolating points\n",
    "plt.plot(x,f,'bo')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-1.5,2.5)\n",
    "plt.title('$c={}$: $v ={}p_0 + {}p_1 + {}p_2$'.format(c,c[0],c[1],c[2]))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least Squares problems: \n",
    "\n",
    "Given the value of a function at any three distinct points is sufficient to describe uniquely the interpolating quadratic through those points.  But suppose we were given more than 3 points, say 7, in which case the matrix $A$ would be $7\\times3$ with rank $r=3$  and unless those 7 points were on the same parabola,  there would be no solution to the overdetermined problem.  Here we will create that problem by adding more points to the interpolating parabola calculated above and then perturb it with uniform random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose 7 evenly spaced points in [-1,1]\n",
    "x = np.linspace(-1,1,7)\n",
    "\n",
    "# perturb the parabola with uniform random noise\n",
    "f = v(c,x) + np.random.uniform(-.5,.5,len(x))\n",
    "\n",
    "# and plot with respect to the underlying parabola\n",
    "plt.figure()\n",
    "plt.plot(x,f,'bo')\n",
    "plt.hold(True)\n",
    "plt.plot(xx,v(c,xx),'r',label='v')\n",
    "plt.xlabel('x')\n",
    "plt.ylim(-1.5,2.5)\n",
    "plt.grid()\n",
    "\n",
    "# now calculate and plot the leastsquares solution to Ac = f\n",
    "c_ls,res,rank,s = la.lstsq(A(x),f)\n",
    "\n",
    "plt.plot(xx,v(c_ls,xx),'g',label='v_lstsq')\n",
    "plt.title('$c={}$: $v={}p_0 + {}p_1 + {}p_2$'.format(c_ls,c_ls[0],c_ls[1],c_ls[2]))\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# and show that this is the same solution we would get if we tried to solve the normal equations direction\n",
    "\n",
    "AtA = np.dot(A(x).transpose(),A(x))\n",
    "Atf = np.dot(A(x).transpose(),f)\n",
    "\n",
    "c_norm = la.solve(AtA,Atf)\n",
    "\n",
    "print 'numpy least-squares c = {}'.format(c_ls)\n",
    "print 'normal equations      = {}'.format(c_norm)\n",
    "print 'difference            = {}'.format(c_ls-c_norm)\n",
    "\n",
    "print\n",
    "print 'ATA ={}'.format(AtA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Now let's show that the error $e= f(x) - A(x)c$ is orthogonal to the column space of $A$ i.e. $A^T e = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the error vector\n",
    "e = f - v(c_ls,x)\n",
    "\n",
    "print 'error vector\\n e={}\\n'.format(e)\n",
    "\n",
    "# and calculate the matrix vector product A^T e\n",
    "print 'A^T e = {}'.format(np.dot(A(x).transpose(),e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projection of a function onto $P_2[-1,1]$\n",
    "\n",
    "Now let's extend this problem to finding the best fit projection of a continuous function $f(x)$ onto $P_2$.  While we could extend the previous approach by sampling $f(x)$ at a large number of points and calculating the least-squares solution,  we can also solve the continuous problem by changing the definition of the inner product from the dot product in $R^n$ to the inner product for continuous functions\n",
    "\n",
    "$$\n",
    "    <f,g> = \\int_{-1}^{1} fg dx\n",
    "$$\n",
    "\n",
    "However the overall approach remains the same as the discrete least squares problem.  \n",
    "\n",
    "If we now consider a function $u \\in P_2[-1,1]$ such that\n",
    "\n",
    "$$\n",
    "    u(x) = \\sum_i c_i p_i(x)\n",
    "$$ \n",
    "\n",
    "then the continous error (or residual) is given by\n",
    "\n",
    "$$\n",
    "    e(x) = u(x) - f(x)\n",
    "$$\n",
    "\n",
    "for the continuous variable $x\\in[-1,1]$.\n",
    "\n",
    "The least square problem now becomes \"find $\\mathbf{c}\\in R^3$ that minimizes $||e||_{L2}$\", i.e. the  \"length\" of $e$ in the $L^2$ norm.  Alternatively this requires that the error $e(x)$ is orthogonal to all the basis vectors in $P_2$, i.e.   \n",
    "\n",
    "$$\n",
    " <p_i,e> = 0 \\quad \\mathrm{for\\, }i=0,1,2\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "    \\int_{-1}^{1} p_i e dx = \\int_{-1}^{1} p_i ( u - f) dx = 0\n",
    "$$\n",
    "or solve \n",
    "$$\n",
    "     \\int_{-1}^{1} p_i \\left(\\sum_j c_j p_j(x)\\right)dx =  \\int_{-1}^{1} p_i f dx\n",
    "$$ \n",
    "\n",
    "for all $i,j=0,1,2$.  Rearranging the summation and the integral sign, we can rewrite the problem as \n",
    "\n",
    "$$\n",
    "    \\sum_j M_{ij} c_j = \\hat{f}_i\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    M_{ij} = <p_i,p_j>=\\int_{-1}^{1} p_i p_j dx\\quad \\mathrm{and}\\quad \\hat{f}_i = <p_i,f> = \\int_{-1}^{1} p_i f dx\n",
    "$$ \n",
    "\n",
    "or in matrix vector notation $M\\mathbf{c} = \\hat{\\mathbf{f}}$ where $M$ is the \"mass-matrix (and corresponds to the symmetric matrix $A^TA$) and $\\hat{\\mathbf{f}}$ is the \"load vector\" which corresponds to  $A^t\\mathbf{b}$.\n",
    "\n",
    "For the simple monomial basis, we can calculate the terms of $M$ easily, but here we will just use scipy's numerical quadrature routines\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining our function and calculating its *interpolation* onto $P_2[-1,1]$ as the unique quadratic that interpolates $f(x)$ at $x=[-1,0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  set the function to be projected\n",
    "f = lambda x : np.cos(2*x) + np.sin(1.5*x)\n",
    "\n",
    "# calculate the interpolation of f onto P2, when sampled at points -1,0,1\n",
    "x = np.array([-1., 0., 1.])\n",
    "c_interp = la.solve(A(x),f(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the mass matrix and load vector and solve for the galerkin projection of $f$ onto  $P_2[-1,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def mij(i,j,x):\n",
    "    \"\"\" integrand for component Mij of the mass matrix\"\"\"\n",
    "    p = np.array([1., x, x**2])\n",
    "    return p[i]*p[j]\n",
    "\n",
    "def fi(i,x,f):\n",
    "    \"\"\" integrand for component i of the load vector\"\"\"\n",
    "    p = np.array([1., x, x**2])\n",
    "    return p[i]*f(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# construct the symmetric mass matrix  M_ij = <p_i,p_j> \n",
    "\n",
    "M = np.zeros((3,3))\n",
    "fhat = np.zeros(3)\n",
    "R = np.zeros((3,3)) # quadrature residuals\n",
    "\n",
    "# loop over the upper triangular elements of M (and fill in the symmetric parts)\n",
    "for i in range(0,3):\n",
    "    fhat[i] = quad(lambda x: fi(i,x,f),-1.,1.)[0]\n",
    "    for j in range(i,3):\n",
    "        result = quad(lambda x: mij(i,j,x),-1.,1.)\n",
    "        M[i,j] = result[0]\n",
    "        M[j,i] = M[i,j]\n",
    "        R[i,j] = result[1]\n",
    "        R[j,i] = R[i,j]\n",
    "        \n",
    "        \n",
    "print 'M = {}\\n'.format(M)\n",
    "print 'fhat = {}\\n'.format(fhat)        \n",
    "\n",
    "# and solve for c\n",
    "c_galerkin = la.solve(M,fhat)\n",
    "\n",
    "print 'c_galerkin ={}'.format(c_galerkin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's just plot out the three function $f(x)$, $f_{interp}(x)$ it's interpolant, and $u(x)$ it's projection onto $P_2[-1,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now plot them all out and compare\n",
    "plt.figure()\n",
    "plt.plot(xx,f(xx),'r',label='$f(x)$')\n",
    "plt.hold(True)\n",
    "plt.plot(x,f(x),'ro')\n",
    "plt.plot(xx,v(c_interp,xx),'g',label='$f_{interp}(x)$')\n",
    "plt.plot(xx,v(c_galerkin,xx),'b',label='$u(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
